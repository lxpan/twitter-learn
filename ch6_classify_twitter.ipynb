{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin\n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "class NLTKBOW(TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    \"\"\" \n",
    "    returns a list of dictionaries, where first dict is list of words in first tweet, and so on\n",
    "    \n",
    "    Key: a word\n",
    "    Value: True or false depending if words was discovered\n",
    "    \"\"\"\n",
    "    def transform(self, X):\n",
    "        return [{word: True for word in word_tokenize(document)} for document in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.naive_bayes import BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "input_filename = os.path.join(os.path.expanduser(\"~\"), \"Data\", \"research\", \"python_tweets.json\")\n",
    "labels_filename = os.path.join(os.path.expanduser(\"~\"), \"Data\", \"research\", \"python_classes.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're interested in the tweets themselves (not the IDs)\n",
    "import json\n",
    "\n",
    "tweets = []\n",
    "with open(input_filename) as inf:\n",
    "    for line in inf:\n",
    "        if len(line.strip()) == 0:\n",
    "            continue\n",
    "        tweets.append(json.loads(line)['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(labels_filename) as inf:\n",
    "    labels = json.load(inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Create a pipeline that has the three components \n",
    "1. The NLTKBOW transformer that has been created\n",
    "2. A DictVectorizer transformer\n",
    "3. A BernoulliNB classifier\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline([('bag-of-words', NLTKBOW()),\n",
    "                    ('vectorizer', DictVectorizer()),\n",
    "                    ('naive-bayes', BernoulliNB())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.950\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "scores = cross_val_score(pipeline, tweets, labels, scoring='f1')\n",
    "print(\"Score: {:.3f}\".format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\nScore: 0.642 -> 0.826\\nTweets: 100\\nComments: a lot of non-English tweets that could not be accurately classified\\n'"
      ]
     },
     "execution_count": 23,
     "output_type": "execute_result",
     "metadata": {}
    }
   ],
   "source": [
    "\"\"\" \n",
    "Score: 0.642 -> 0.826\n",
    "Tweets: 100\n",
    "Comments: a lot of non-English tweets that could not be accurately classified\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the best features for determining if a tweet is relevant or not?\n",
    "\n",
    "# Fit our pipeline with the tweets - creating a new model\n",
    "\n",
    "model = pipeline.fit(tweets, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = model.named_steps['naive-bayes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_probabilities = nb.feature_log_prob_\n",
    "top_features = np.argsort(-feature_probabilities[1])[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map feature's indices (from prev step) to the actual values\n",
    "\n",
    "dv = model.named_steps['vectorizer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : 0.820143884892\n",
      "1 http 0.73381294964\n",
      "2 Python 0.661870503597\n",
      "3 # 0.402877697842\n",
      "4 and 0.31654676259\n",
      "5 with 0.309352517986\n",
      "6 the 0.294964028777\n",
      "7 , 0.273381294964\n",
      "8 a 0.26618705036\n",
      "9 to 0.258992805755\n",
      "10 @ 0.244604316547\n",
      "11 for 0.237410071942\n",
      "12 https 0.215827338129\n",
      "13 . 0.201438848921\n",
      "14 python 0.201438848921\n",
      "15 in 0.165467625899\n",
      "16 is 0.158273381295\n",
      "17 ... 0.143884892086\n",
      "18 I 0.136690647482\n",
      "19 How 0.129496402878\n",
      "20 Automate 0.122302158273\n",
      "21 Boring 0.122302158273\n",
      "22 Stuff 0.122302158273\n",
      "23 - 0.122302158273\n",
      "24 you 0.115107913669\n",
      "25 Scapy 0.107913669065\n",
      "26 Build 0.107913669065\n",
      "27 Stealth 0.107913669065\n",
      "28 Port 0.107913669065\n",
      "29 're 0.107913669065\n",
      "30 As 0.107913669065\n",
      "31 we 0.107913669065\n",
      "32 ? 0.107913669065\n",
      "33 Scanner 0.107913669065\n",
      "34 The 0.0791366906475\n",
      "35 ! 0.0791366906475\n",
      "36 from 0.0791366906475\n",
      "37 ( 0.0719424460432\n",
      "38 ) 0.0719424460432\n",
      "39 via 0.0647482014388\n",
      "40 can 0.0647482014388\n",
      "41 Learn 0.0647482014388\n",
      "42 framework 0.0575539568345\n",
      "43 Data 0.0575539568345\n",
      "44 it 0.0575539568345\n",
      "45 that 0.0575539568345\n",
      "46 this 0.0575539568345\n",
      "47 my 0.0503597122302\n",
      "48 aware 0.0503597122302\n",
      "49 all 0.0503597122302\n"
     ]
    }
   ],
   "source": [
    "for i, feature_index in enumerate(top_features):\n",
    "    print(i, dv.feature_names_[feature_index],\n",
    "          np.exp(feature_probabilities[1][feature_index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/lpan/Models/twitter/python_context.pkl'"
      ]
     },
     "execution_count": 29,
     "output_type": "execute_result",
     "metadata": {}
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "output_filename = os.path.join(os.path.expanduser(\"~\"), \"Models\", \"twitter\", \"python_context.pkl\")\n",
    "output_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/lpan/Models/twitter/python_context.pkl',\n",
       " '/Users/lpan/Models/twitter/python_context.pkl_01.npy',\n",
       " '/Users/lpan/Models/twitter/python_context.pkl_02.npy',\n",
       " '/Users/lpan/Models/twitter/python_context.pkl_03.npy',\n",
       " '/Users/lpan/Models/twitter/python_context.pkl_04.npy',\n",
       " '/Users/lpan/Models/twitter/python_context.pkl_05.npy']"
      ]
     },
     "execution_count": 30,
     "output_type": "execute_result",
     "metadata": {}
    }
   ],
   "source": [
    "joblib.dump(model, output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}